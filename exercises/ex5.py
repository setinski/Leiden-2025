import numpy as np

##################
# A few numpy subtleties to discuss before-hand:
# - numpy arrays are "passed by pointer": in place modification
# - submatrix extraction is magic: no copies done (pointers and "strides")
# - This means that modifying the submatrix modifies the original matrix
# - include diagonal extraction ! 
# - Beware and make explicit copies when needed

##################


# In the first tutorial, we implemented various lattice algorithm, 
# using function interface and representation of object trying to
# follow the mathematical formalism of the lecture notes. This effort
# is mostly motivated by pedagogy, but does not represent how such
# algorithm are typically implemented.

# In particular, we re-impelemented by hand many standard algorithm
# such as Gram-Schmidt, while we could be using existing linear algebra
# routine from python's numpy library. Exploiting such heavily optimized
# routines is critical in reaching faster lattice algorithm in practice.

# One particular question that arise remain how to represent the various
# vectors manipulated by the algorithm: the standard basis of R^n, the
# (current) basis B of the lattice, or the Gram-Schmidt basis B*. 
# A concern is to avoid useless conversion between those representations
# as each of them might be quite costly. After decades of doing various
# implementation of this algorithm the following idiom have emerged for
# function interface:

# A/ The basis B itself is represented in the cannonical basis
# B/ Lattice vectors are then represented in the current basis B,
#    using integer coordinates
# C/ arbitrary vectors from Span(B), such as BDD/CVP targets 
#   are represented in basis B*, using floating points coordinates

# Some nice features of those reprensetation:
# B/ a vector of the sublattice generated by B_{i:j} can be embedded
# in B_{i:k} for k >= j simply by padding it with k-j many 0's.
# C/ Projecting a vector from Span(B_{i:j}) to Span(B_{k:j}) for 
# i <= k <=j can be done but deleting (or just ignoring) the first
# k - i coordinate.

# For dealing with the Gram-Schmidt decomposition, we normally
# rarely need to use Q. A nice feature of this decomposition is
# that the row of L[i] in the decomposition B = L * D * Q exactly
# gives representation of B[i] in basis Bs. This is particularly
# convenient in combination with the representation C/ discussed
# above.

# For the diagonal matrix D, we will represent it by a mere vector,
# The multiplication by the matrix D can be done by a mere 
# "coefficient-wise" multiplication by that vector: "D * t" in numpy.

def gram_schmidt_decomposition(B):
	"""
	Take as input a basis B and return its Gram Schmidt decomposition.
	:param B: an (n x d) Matrix B (n <= d)
	:return: L, D, Q such that B = LDQ, where 
	- L is Lower Triangular	with unit diagonal, 
	- D is a diagonal matrix (represented as just a vector)
	- and Q is orthogonal.
	"""
	# For some reason numpy offers qr but not lq. Use Transposition.
	Q,R = np.linalg.qr(B.T)
	L = R.T
	D = np.diag(L)
	L = L @ np.diag(1/D)
	return L, D, Q.T

### Sanity check the equation
B = np.array([[1,2,3],[9,2,0],[0,8,3]], dtype="int")
L, D, Q = gram_schmidt_decomposition(B)
assert np.allclose(B, L @ np.diag(D) @ Q)
### Note: more checks on L, D, Q having the right properties 
### should be added.

def nearest_plane(L, t):
	"""
	Take as input the L of a Gram Schmidt Decomposition,
	and a vector t expressed in basis B* = DQ. Modifies the target 
	t in place, and return a lattice vector v expressed in base
	B.

	That is : t_old @ Bs = t_new @ Bs + v @ B, ensuring that 
	t_new has all its coordinate in the range [-1/2, 1/2].
	"""
	n = len(t)
	v = np.zeros(n, dtype="int")
	pass

### Sanity check the equation
t_old = np.array([4.2, 5.3, 10.3])
t_new = np.copy(t_old)
v = nearest_plane(L, t_new)
assert(np.allclose((t_old * D) @ Q, ((t_new * D) @ Q) + (v @ B)))
assert(np.max(np.abs(t_new)) <= .5)

def size_reduce(B, L):
	"""
	Size reduce B, consistently modifying B and L in place.
	No return value.
	"""	
	n, _ = B.shape
	pass

B_old = np.copy(B)
size_reduce(B, L)
L2, D2, Q2 = gram_schmidt_decomposition(B)
assert(np.allclose(L, L2))
assert(np.allclose(D, D2))
assert(np.allclose(Q, Q2))
U = B_old @ np.linalg.inv(B)
U_ = np.round(U)
assert(np.allclose(U, U_))
assert(np.linalg.det(U) == 1.)




### For LLL, we can implement something simpler: instead of
# applying a full Lagrange-reduction when Lovasz condition
# is not satisfied, we can instead only do a simple swap;
# which corresponds to a single step of the local
# Lagrange-reduction. This is the typical way that LLL is
# implemented (and how it was originally presented).


def slow_LLL(B, epsilon=0.01):
	n,_ = B.shape
	while True:
		pass
	
		yield list(np.log(np.abs(D)))
		
		pass


def LLL(B, epsilon=0.01):
	n,_ = B.shape
	changed = True
	while changed:
		pass
	
		yield list(np.log(np.abs(D)))
		
		pass




###############
import matplotlib.pyplot as plt
import matplotlib.animation as animation

def anim_LLL(n, q, slow=True):
	B = np.identity(n, dtype=int)
	B[0, 0] = q
	for i in range(1, n):
		B[i, 0] = np.random.randint(0, q)

	B_old = np.copy(B)
	data = list(slow_LLL(B)) if slow else list(LLL(B))
	U = B_old @ np.linalg.inv(B)
	U_ = np.round(U)
	assert(np.allclose(U, U_))
	U = B @ np.linalg.inv(B_old)
	U_ = np.round(U)
	assert(np.allclose(U, U_))

	for i in range(40):
		data.append(data[-1])

	fig, ax = plt.subplots()
	line2 = ax.plot(range(n), data[0], label="basis profile")[0]
	ax.set(xlim=[0, n], ylim=[0, np.log(q)/4])
	ax.legend()

	def update_anim(frame):
		line2.set_ydata(data[frame])
		return line2

	return animation.FuncAnimation(fig=fig,func=update_anim, frames=len(data), interval=50)

############
# Main runner
############
if __name__ == "__main__":
    ani = anim_LLL(n=30, q=9999999)
    plt.show()
    ani = anim_LLL(n=30, q=9999999, slow=False)
    plt.show()