import numpy as np
from math import sqrt, log

##################
# A few numpy subtleties to discuss before-hand:
# - numpy arrays are "passed by pointer": in place modification
# - submatrix extraction is magic: no copies done (pointers and "strides")
# - This means that modifying the submatrix modifies the original matrix
# - include diagonal extraction ! 
# - Beware and make explicit copies when needed
#  np.diag(x)@y
##################


# In the first tutorial, we implemented various lattice algorithm, 
# using function interface and representation of object trying to
# follow the mathematical formalism of the lecture notes. This effort
# is mostly motivated by pedagogy, but does not represent how such
# algorithm are typically implemented.

# In particular, we re-impelemented by hand many standard algorithm
# such as Gram-Schmidt, while we could be using existing linear algebra
# routine from python's numpy library. Exploiting such heavily optimized
# routines is critical in reaching faster lattice algorithm in practice.

# One particular question that arise remain how to represent the various
# vectors manipulated by the algorithm: the standard basis of R^n, the
# (current) basis B of the lattice, or the Gram-Schmidt basis B*. 
# A concern is to avoid useless conversion between those representations
# as each of them might be quite costly. After decades of doing various
# implementation of this algorithm the following idiom have emerged for
# function interface:

# A/ The basis B itself is represented in the cannonical basis
# B/ Lattice vectors are then represented in the current basis B,
#    using integer coordinates
# C/ arbitrary vectors from Span(B), such as BDD/CVP targets 
#   are represented in basis B*, using floating points coordinates

# Some nice features of those reprensetation:
# B/ a vector of the sublattice generated by B_{i:j} can be embedded
# in B_{i:k} for k >= j simply by padding it with k-j many 0's.
# C/ Projecting a vector from Span(B_{i:j}) to Span(B_{k:j}) for 
# i <= k <=j can be done but deleting (or just ignoring) the first
# k - i coordinate.

# For dealing with the Gram-Schmidt decomposition, we normally
# rarely need to use Q. A nice feature of this decomposition is
# that the row of L[i] in the decomposition B = L * D * Q exactly
# gives representation of B[i] in basis Bs. This is particularly
# convenient in combination with the representation C/ discussed
# above.

# For the diagonal matrix D, we will represent it by a mere vector,
# The multiplication by the matrix D can be done by a mere 
# "coefficient-wise" multiplication by that vector: "D * t" in numpy.

def gram_schmidt_decomposition(B):
    n=B.shape[0]
    Qfirst,R=np.linalg.qr(B.T)
    Q=Qfirst.T 
    Low=R.T
    assert(np.allclose(B, Low @ Q))
    D=np.copy(np.diag(Low))	
    L=np.copy(Low)
    for i in range(n):
        L[:,i]=Low[:,i]/D[i]
    return L,D,Q

    """
	Take as input a basis B and return its Gram Schmidt decomposition.
	:param B: an (n x d) Matrix B (n <= d)
	:return: L, D, Q such that B = LDQ, where 
	- L is Lower Triangular	with unit diagonal, 
	- D is a diagonal matrix (represented as just a vector)
	- and Q is orthogonal.
	"""

	# For some reason numpy offers qr but not lq. Use Transposition.

### Sanity check the equation
B = np.array([[1,2,3],[9,2,0],[0,8,3]], dtype="int")
L, D, Q = gram_schmidt_decomposition(B)
assert np.allclose(B, L @ np.diag(D) @ Q)
print(L)

n=B.shape[0]

for i in range(n): #checking whether L is low. tri.
    for j in range(n):
        if (j> i):
            assert np.allclose(L[i,j], 0)

for j in range(n): #checking whether Q is orthogonal
    assert np.allclose(Q[:,j]@ Q[:,j],1)
    for k in range(j+1, n):
        assert np.allclose(Q[:,j]@ Q[:,k],0)

def nearest_plane(L, t):
    """
	Take as input the L of a Gram Schmidt Decomposition,
	and a vector t expressed in basis B* = DQ. Modifies the target 
	t in place, and return a lattice vector v expressed in base
	B.

	That is : t_old @ Bs = t_new @ Bs + v @ B, ensuring that 
	t_new has all its coordinate in the range [-1/2, 1/2].
    """
    n = len(t)
    v = np.zeros(n, dtype="int")
    for i in reversed(range(n)):
        k=int(np.round(t[i]))
        v[i]=k
        t-=k*L[i]
    return v

### Sanity check the equation
t_old = np.array([4.2, 5.3, 10.3])
t_new = np.copy(t_old)
v = nearest_plane(L, t_new)
assert(np.allclose((t_old * D) @ Q, ((t_new * D) @ Q) + (v @ B)))
assert(np.max(np.abs(t_new)) <= .5)

def size_reduce(B, L):
    """
	Size reduce B, consistently modifying B and L in place.
	No return value.
    """	
    n= B.shape[0]
    for i in range(n):
        v=nearest_plane(L[:i, :i], L[i,:i])   ## L[i,:i] are the coeff L[i,0], L[i,1],..., L[i,i-1], len(v)=i
        B[i]-=v@B[:i] 
    return


B_old = np.copy(B)
size_reduce(B, L)
L2, D2, Q2 = gram_schmidt_decomposition(B)
assert(np.allclose(L, L2))
assert(np.allclose(D, D2))
assert(np.allclose(Q, Q2))
U = B_old @ np.linalg.inv(B)
U_ = np.round(U)
assert(np.allclose(U, U_))
assert(np.linalg.det(U) == 1.)




### For LLL, we can implement something simpler: instead of
# applying a full Lagrange-reduction when Lovasz condition
# is not satisfied, we can instead only do a simple swap;
# which corresponds to a single step of the local
# Lagrange-reduction. This is the typical way that LLL is
# implemented (and how it was originally presented).


def slow_LLL(B, epsilon=0.01):
    max_iter=1000
    n=B.shape[0]
    L,D,Q=gram_schmidt_decomposition(B)
    size_reduce(B,L)

    for _ in range(max_iter):
        for i in range(n-1):
            if np.linalg.norm(Bs[i]) > (sqrt(4/3) + epsilon) * np.linalg.norm(Bs[i+1]):
                B[[i, i + 1]] = B[[i + 1, i]]
                L,D,Q=gram_schmidt_decomposition(B)
                size_reduce(B, L)

    raise RuntimeError("LLL did not converge within the maximum number of iterations.")
	#"yield list(np.log(np.abs(D)))" don't know what to do with this


def LLL(B, epsilon=0.01):
    max_iter=1000
    n=B.shape[0]
    L,D,Q=gram_schmidt_decomposition(B)
    Bs=np.diag(D)@Q

    for _ in range(max_iter):
        changed=False
        for i in range(n-1):
            if np.linalg.norm(Bs[i]) > (sqrt(4/3) + epsilon) * np.linalg.norm(Bs[i+1]):
                B[[i, i + 1]] = B[[i + 1, i]]
                L,D,Q=gram_schmidt_decomposition(B)
                Bs=np.diag(D)@Q
                size_reduce(B, L)
                changed=True

        if not changed:
            return

    raise RuntimeError("LLL did not converge within the maximum number of iterations.")
	#"yield list(np.log(np.abs(D)))" don't know what to do with this




###############
import matplotlib.pyplot as plt
import matplotlib.animation as animation

def anim_LLL(n, q, slow=True):
	B = np.identity(n, dtype=int)
	B[0, 0] = q
	for i in range(1, n):
		B[i, 0] = np.random.randint(0, q)

	B_old = np.copy(B)
	data = list(slow_LLL(B)) if slow else list(LLL(B))
	U = B_old @ np.linalg.inv(B)
	U_ = np.round(U)
	assert(np.allclose(U, U_))
	U = B @ np.linalg.inv(B_old)
	U_ = np.round(U)
	assert(np.allclose(U, U_))

	for i in range(40):
		data.append(data[-1])

	fig, ax = plt.subplots()
	line2 = ax.plot(range(n), data[0], label="basis profile")[0]
	ax.set(xlim=[0, n], ylim=[0, np.log(q)/4])
	ax.legend()

	def update_anim(frame):
		line2.set_ydata(data[frame])
		return line2

	return animation.FuncAnimation(fig=fig,func=update_anim, frames=len(data), interval=50)

############
# Main runner
############
if __name__ == "__main__":
    ani = anim_LLL(n=30, q=9999999)
    plt.show()
    ani = anim_LLL(n=30, q=9999999, slow=False)
    plt.show()